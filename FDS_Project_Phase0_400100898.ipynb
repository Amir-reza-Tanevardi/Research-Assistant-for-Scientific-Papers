{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "In The Name of God\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Sharif University of Technology - Department of Electrical Engineering\n",
        "</font>\n",
        "<br>\n",
        "<font color=\"#008080\" size=6>\n",
        "Foundations of Data Science\n",
        "</font>\n",
        "<hr/>\n",
        "<font color=\"#808000\" size=5>\n",
        "Phase 0 Report and Code\n",
        "<br>\n",
        "</font>\n",
        "<font size=5>\n",
        "Instructor: Dr. Khalaj\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Fall 2024\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "Amirreza Tanevardi 400100898\n",
        "</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "2t-KEVWQQzVx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnSjMLz9-xsg"
      },
      "source": [
        "# Crawling:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Function to fetch research papers from the Semantic Scholar API\n",
        "def fetch_papers(topic, year_range, max_papers=1000):\n",
        "    \"\"\"\n",
        "    Retrieves research papers from the Semantic Scholar API based on a given topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The research topic to search for.\n",
        "        year_range (str): The range of publication years (e.g., \"2017-2023\").\n",
        "        max_papers (int): The maximum number of papers to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing paper details (title, abstract, authors, citations).\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    papers_collected = []\n",
        "    offset = 0\n",
        "\n",
        "    while len(papers_collected) < max_papers:\n",
        "        if offset % 300 == 0:\n",
        "            print(f\"Progress: {len(papers_collected)}/{max_papers}\")\n",
        "\n",
        "        # Define API request parameters\n",
        "        params = {\n",
        "            \"query\": topic,\n",
        "            \"fields\": \"title,abstract,authors,citationCount\",\n",
        "            \"offset\": offset,\n",
        "            \"limit\": 100,\n",
        "            \"year\": year_range,\n",
        "            \"sort\": \"relevance\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, params=params)\n",
        "\n",
        "            # Handle rate limit restrictions\n",
        "            if response.status_code == 429:\n",
        "                print(\"Rate limit exceeded. Waiting for 10 seconds...\")\n",
        "                time.sleep(10)\n",
        "                continue\n",
        "\n",
        "            # Handle non-successful responses\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error {response.status_code}: {response.json()}\")\n",
        "                break\n",
        "\n",
        "            response_data = response.json()\n",
        "            paper_entries = response_data.get(\"data\", [])\n",
        "\n",
        "            # Stop if no further papers are found\n",
        "            if not paper_entries:\n",
        "                print(\"No more papers available.\")\n",
        "                break\n",
        "\n",
        "            # Process and store paper details\n",
        "            for paper in paper_entries:\n",
        "                try:\n",
        "                    papers_collected.append({\n",
        "                        'title': paper.get(\"title\", \"No Title\"),\n",
        "                        'abstract': paper.get(\"abstract\", \"No Abstract\"),\n",
        "                        'authors': \", \".join([author.get(\"name\", \"Unknown\") for author in paper.get(\"authors\", [])]),\n",
        "                        'citations': paper.get(\"citationCount\", 0)\n",
        "                    })\n",
        "\n",
        "                    # Stop if the paper limit is reached\n",
        "                    if len(papers_collected) >= max_papers:\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing paper details: {e}\")\n",
        "\n",
        "            # Move to the next batch of results\n",
        "            offset += 100\n",
        "            time.sleep(5)  # Avoid overwhelming the server\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error: {e}. Retrying in 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    return papers_collected\n",
        "\n",
        "# Function to store scraped research papers into a JSON file\n",
        "def store_in_json(topic, collected_data):\n",
        "    \"\"\"\n",
        "    Saves research papers to a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The research topic.\n",
        "        collected_data (list): The gathered research papers categorized by year range.\n",
        "    \"\"\"\n",
        "    if not collected_data:\n",
        "        print(f\"No data available for topic: {topic}\")\n",
        "        return\n",
        "\n",
        "    # Generate a valid filename based on the topic\n",
        "    filename = f\"{topic.replace(' ', '_')}_papers.json\"\n",
        "\n",
        "    # Save the research data into a formatted JSON file\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(collected_data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Data successfully saved to {filename}\")\n",
        "\n",
        "# Topics and year ranges for data collection\n",
        "topics = [\"Foundation Models\", \"Generative Models\", \"LLM\", \"VLM\", \"Diffusion Models\"]\n",
        "year_ranges = [\"2017-2020\", \"2021-2023\", \"2024-2025\"]\n",
        "\n",
        "# Iterate over topics and fetch relevant research papers\n",
        "for topic in topics:\n",
        "    print(f\"Starting data collection for topic: {topic}\")\n",
        "    compiled_data = []\n",
        "\n",
        "    # Retrieve papers for each specified year range\n",
        "    for year_range in year_ranges:\n",
        "        papers = fetch_papers(topic, year_range, max_papers=1000)\n",
        "        compiled_data.append({'year_range': year_range, 'papers': papers})\n",
        "\n",
        "    # Store the fetched data into a JSON file\n",
        "    store_in_json(topic, compiled_data)\n",
        "    print(f\"Completed data collection for topic: {topic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGZaKc5MN146",
        "outputId": "3d1da04f-85c0-4fd3-cdd9-9b851e87242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for topic: Foundation Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Foundation_Models_data.json\n",
            "Completed scraping for topic: Foundation Models\n",
            "Scraping data for topic: Generative Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 0/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 900/1000\n",
            "Data saved to Generative_Models_data.json\n",
            "Completed scraping for topic: Generative Models\n",
            "Scraping data for topic: LLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 825/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to LLM_data.json\n",
            "Completed scraping for topic: LLM\n",
            "Scraping data for topic: VLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 544/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to VLM_data.json\n",
            "Completed scraping for topic: VLM\n",
            "Scraping data for topic: Diffusion Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Diffusion_Models_data.json\n",
            "Completed scraping for topic: Diffusion Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Report"
      ],
      "metadata": {
        "id": "JHeCmD0-04c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a detailed report explaining how the provided scraping code works:\n",
        "\n",
        "---\n",
        "\n",
        "# **Report on the Research Paper Scraping Code**\n",
        "\n",
        "## **1. Overview**  \n",
        "This Python script is designed to collect research papers related to specific topics from the **Semantic Scholar API**. It searches for papers within defined publication year ranges, processes the retrieved data, and saves the results in JSON files for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Components**\n",
        "\n",
        "### **a. Libraries Used**\n",
        "- **`requests`**: Handles HTTP requests to the Semantic Scholar API.\n",
        "- **`time`**: Manages delays between requests to prevent rate-limiting issues.\n",
        "- **`json`**: Formats and saves the collected data as JSON files.\n",
        "- **`os`**: (Imported but unused in the script.)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Functions Explained**\n",
        "\n",
        "### **a. `fetch_papers(topic, year_range, max_papers=1000)`**\n",
        "\n",
        "- **Purpose:**  \n",
        "  Retrieves research papers based on a given topic and publication year range from the Semantic Scholar API.\n",
        "\n",
        "- **Parameters:**  \n",
        "  - `topic` *(str)*: The research topic (e.g., \"LLM\", \"Diffusion Models\").  \n",
        "  - `year_range` *(str)*: The publication years to filter results (e.g., \"2017-2020\").  \n",
        "  - `max_papers` *(int, default=1000)*: The maximum number of papers to collect.\n",
        "\n",
        "- **Process:**  \n",
        "  1. **API Request Setup:**  \n",
        "     Uses the Semantic Scholar API with query parameters:\n",
        "     - `query`: Topic of interest.\n",
        "     - `fields`: Retrieves specific fields (title, abstract, authors, citations).\n",
        "     - `offset` and `limit`: Handles pagination (fetches 100 papers per request).\n",
        "     - `year`: Limits papers to the specified year range.\n",
        "     - `sort`: Sorts by relevance.\n",
        "\n",
        "  2. **Handling API Rate Limits:**  \n",
        "     If the API returns status code **429** (rate limit exceeded), the script waits for 10 seconds before retrying.\n",
        "\n",
        "  3. **Error Handling:**  \n",
        "     - Prints an error message for non-200 HTTP status codes.\n",
        "     - Handles exceptions during API requests and retries after a 10-second delay.\n",
        "\n",
        "  4. **Data Processing:**  \n",
        "     Extracts key details from each paper:\n",
        "     - **Title**\n",
        "     - **Abstract**\n",
        "     - **Authors** (joined into a single string)\n",
        "     - **Citation Count**\n",
        "\n",
        "  5. **Progress Tracking:**  \n",
        "     Displays progress every 300 papers collected.\n",
        "\n",
        "  6. **Termination Conditions:**  \n",
        "     - Stops if no more papers are available.\n",
        "     - Stops once the maximum paper limit (`max_papers`) is reached.\n",
        "\n",
        "---\n",
        "\n",
        "### **b. `store_in_json(topic, collected_data)`**\n",
        "\n",
        "- **Purpose:**  \n",
        "  Saves the collected research papers into a JSON file.\n",
        "\n",
        "- **Parameters:**  \n",
        "  - `topic` *(str)*: The research topic (used for naming the output file).\n",
        "  - `collected_data` *(list)*: The list of papers categorized by year range.\n",
        "\n",
        "- **Process:**  \n",
        "  1. **File Naming:**  \n",
        "     Replaces spaces in the topic with underscores to create a valid filename (e.g., `Diffusion_Models_papers.json`).\n",
        "\n",
        "  2. **Data Storage:**  \n",
        "     Saves the data in JSON format with proper indentation for readability.\n",
        "\n",
        "  3. **Output Confirmation:**  \n",
        "     Prints a success message once the data is saved.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Main Execution Flow**\n",
        "\n",
        "### **Topics and Year Ranges:**\n",
        "- **Topics:**  \n",
        "  - *\"Foundation Models\"*, *\"Generative Models\"*, *\"LLM\"*, *\"VLM\"*, *\"Diffusion Models\"*\n",
        "\n",
        "- **Year Ranges:**  \n",
        "  - *\"2017-2020\"*, *\"2021-2023\"*, *\"2024-2025\"*\n",
        "\n",
        "### **Steps:**\n",
        "1. **Iterate Over Topics:**  \n",
        "   For each topic, the script:\n",
        "   - Prints a message indicating the start of data collection.\n",
        "   - Initializes an empty list `compiled_data` to store papers.\n",
        "\n",
        "2. **Fetch Papers for Each Year Range:**  \n",
        "   Calls `fetch_papers()` for each specified year range (up to 1000 papers per range).\n",
        "\n",
        "3. **Save Data:**  \n",
        "   Calls `store_in_json()` to save the compiled data into a JSON file.\n",
        "\n",
        "4. **Completion Message:**  \n",
        "   Prints a message indicating the completion of data collection for the topic.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Features and Error Handling**\n",
        "\n",
        "- **Rate Limiting:**  \n",
        "  Handles API throttling by pausing for 10 seconds when needed.\n",
        "\n",
        "- **Robust Error Handling:**  \n",
        "  Catches both request-related and data-processing errors.\n",
        "\n",
        "- **Progress Monitoring:**  \n",
        "  Displays real-time progress updates.\n",
        "\n",
        "- **Organized Data Storage:**  \n",
        "  Saves data in JSON files categorized by topic and year range.\n",
        "\n"
      ],
      "metadata": {
        "id": "giMuS7lM07Do"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}